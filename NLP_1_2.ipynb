{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1) Text Preprocessing with NLTK and spaCy"
      ],
      "metadata": {
        "id": "B7CbseWtupNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLTK solution"
      ],
      "metadata": {
        "id": "yyn01UkaXyAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "sample_text = \"\"\"Keith recently came back from a trip to Chicago, Illinois. This midwestern metropolis is found along the shore of Lake Michigan.\n",
        "During his visit, Keith spent a lot of time exploring the city to visit important landmarks and monuments.\"\"\"\n",
        "\n",
        "tokenized_text_nltk = word_tokenize(sample_text)\n",
        "lemmatized_text_nltk = [nltk.stem.WordNetLemmatizer().lemmatize(word) for word in tokenized_text_nltk]\n",
        "stop_words_removed_nltk = [word for word in lemmatized_text_nltk if word not in nltk.corpus.stopwords.words('english')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0UMItESSrkz",
        "outputId": "969c35c3-c8c6-407a-8f4b-1d9566f342e9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"tokenized by NLTK text is: \\n {tokenized_text_nltk}\\n\")\n",
        "print(f\"lemmatized text is: \\n {lemmatized_text_nltk}\\n\")\n",
        "print(f\"without stop words text is: \\n {stop_words_removed_nltk}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9khJix5Vpq0",
        "outputId": "d07d2680-075c-4dfe-c61c-07375d1857fb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized by NLTK text is: \n",
            " ['Keith', 'recently', 'came', 'back', 'from', 'a', 'trip', 'to', 'Chicago', ',', 'Illinois', '.', 'This', 'midwestern', 'metropolis', 'is', 'found', 'along', 'the', 'shore', 'of', 'Lake', 'Michigan', '.', 'During', 'his', 'visit', ',', 'Keith', 'spent', 'a', 'lot', 'of', 'time', 'exploring', 'the', 'city', 'to', 'visit', 'important', 'landmarks', 'and', 'monuments', '.']\n",
            "\n",
            "lemmatized text is: \n",
            " ['Keith', 'recently', 'came', 'back', 'from', 'a', 'trip', 'to', 'Chicago', ',', 'Illinois', '.', 'This', 'midwestern', 'metropolis', 'is', 'found', 'along', 'the', 'shore', 'of', 'Lake', 'Michigan', '.', 'During', 'his', 'visit', ',', 'Keith', 'spent', 'a', 'lot', 'of', 'time', 'exploring', 'the', 'city', 'to', 'visit', 'important', 'landmark', 'and', 'monument', '.']\n",
            "\n",
            "without stop words text is: \n",
            " ['Keith', 'recently', 'came', 'back', 'trip', 'Chicago', ',', 'Illinois', '.', 'This', 'midwestern', 'metropolis', 'found', 'along', 'shore', 'Lake', 'Michigan', '.', 'During', 'visit', ',', 'Keith', 'spent', 'lot', 'time', 'exploring', 'city', 'visit', 'important', 'landmark', 'monument', '.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Spacy solution"
      ],
      "metadata": {
        "id": "a2vOgsdjX-Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tokenize_text(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    return tokens\n",
        "\n",
        "def lemmatize_text(text):\n",
        "  doc = nlp(text)\n",
        "  lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "  return lemmatized_tokens\n",
        "\n",
        "def stop_words_removing(text):\n",
        "  doc = nlp(text)\n",
        "  filtered_tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "  return filtered_tokens\n",
        "\n",
        "tokenized_text_spacy = tokenize_text(sample_text)\n",
        "lemmatized_text_spacy = lemmatize_text(sample_text)\n",
        "stop_words_removed_spacy = stop_words_removing(sample_text)"
      ],
      "metadata": {
        "id": "e3RWl2UaYDAn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"tokenized by SPACY text is: \\n {tokenized_text_spacy}\\n\")\n",
        "print(f\"lemmatized text is: \\n {lemmatized_text_spacy}\\n\")\n",
        "print(f\"without stop words text is: \\n {stop_words_removed_spacy}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZNB8I-7av-I",
        "outputId": "444346aa-8db8-4dd3-ec3e-ad50e47d020e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized by SPACY text is: \n",
            " ['Keith', 'recently', 'came', 'back', 'from', 'a', 'trip', 'to', 'Chicago', ',', 'Illinois', '.', 'This', 'midwestern', 'metropolis', 'is', 'found', 'along', 'the', 'shore', 'of', 'Lake', 'Michigan', '.', '\\n', 'During', 'his', 'visit', ',', 'Keith', 'spent', 'a', 'lot', 'of', 'time', 'exploring', 'the', 'city', 'to', 'visit', 'important', 'landmarks', 'and', 'monuments', '.']\n",
            "\n",
            "lemmatized text is: \n",
            " ['Keith', 'recently', 'come', 'back', 'from', 'a', 'trip', 'to', 'Chicago', ',', 'Illinois', '.', 'this', 'midwestern', 'metropolis', 'be', 'find', 'along', 'the', 'shore', 'of', 'Lake', 'Michigan', '.', '\\n', 'during', 'his', 'visit', ',', 'Keith', 'spend', 'a', 'lot', 'of', 'time', 'explore', 'the', 'city', 'to', 'visit', 'important', 'landmark', 'and', 'monument', '.']\n",
            "\n",
            "without stop words text is: \n",
            " ['Keith', 'recently', 'came', 'trip', 'Chicago', 'Illinois', 'midwestern', 'metropolis', 'found', 'shore', 'Lake', 'Michigan', '\\n', 'visit', 'Keith', 'spent', 'lot', 'time', 'exploring', 'city', 'visit', 'important', 'landmarks', 'monuments']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2) Named Entity Recognition (NER) with spaCy"
      ],
      "metadata": {
        "id": "tbCykc-lu24h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "text = \"Far far away, behind the word mountains, far from the countries Vokalia and Consonantia, there live the blind texts.\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "print(\"Named Entities, Phrases, and Concepts:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} ({ent.label_})\")\n",
        "\n",
        "displacy.serve(doc, style=\"ent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "aErdPBUxuzc9",
        "outputId": "6aff8de2-0f77-41ee-f6a4-4bd2d77cca50"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities, Phrases, and Concepts:\n",
            "Chicago (GPE)\n",
            "Illinois (GPE)\n",
            "Lake Michigan (LOC)\n",
            "Keith (PERSON)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/displacy/__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
            "  warnings.warn(Warnings.W011)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "    <head>\n",
              "        <title>displaCy</title>\n",
              "    </head>\n",
              "\n",
              "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
              "<figure style=\"margin-bottom: 6rem\">\n",
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Keith recently came back from a trip to \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Chicago\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Illinois\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". This midwestern metropolis is found along the shore of \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Lake Michigan\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ".<br>During his visit, \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Keith\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " spent a lot of time exploring the city to visit important landmarks and monuments.</div>\n",
              "</figure>\n",
              "</body>\n",
              "</html></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using the 'ent' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3) Text Vectorization using Transformers"
      ],
      "metadata": {
        "id": "LAHSwO415SAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\", output_hidden_states = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsyClFXO6d9X",
        "outputId": "22f2fcdf-3b15-422d-ab95-25086dab4567"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:818: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(sample_text)\n",
        "encodings = tokenizer.encode(sample_text)"
      ],
      "metadata": {
        "id": "m9Pu4Z-SA7VN"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens, encodings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMTvjAvSBHJi",
        "outputId": "490c2f0d-3b20-4ebf-cea9-a393eb14f1ed"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['keith',\n",
              "  'recently',\n",
              "  'came',\n",
              "  'back',\n",
              "  'from',\n",
              "  'a',\n",
              "  'trip',\n",
              "  'to',\n",
              "  'chicago',\n",
              "  ',',\n",
              "  'illinois',\n",
              "  '.',\n",
              "  'this',\n",
              "  'midwest',\n",
              "  '##ern',\n",
              "  'metropolis',\n",
              "  'is',\n",
              "  'found',\n",
              "  'along',\n",
              "  'the',\n",
              "  'shore',\n",
              "  'of',\n",
              "  'lake',\n",
              "  'michigan',\n",
              "  '.',\n",
              "  'during',\n",
              "  'his',\n",
              "  'visit',\n",
              "  ',',\n",
              "  'keith',\n",
              "  'spent',\n",
              "  'a',\n",
              "  'lot',\n",
              "  'of',\n",
              "  'time',\n",
              "  'exploring',\n",
              "  'the',\n",
              "  'city',\n",
              "  'to',\n",
              "  'visit',\n",
              "  'important',\n",
              "  'landmarks',\n",
              "  'and',\n",
              "  'monuments',\n",
              "  '.'],\n",
              " [101,\n",
              "  6766,\n",
              "  3728,\n",
              "  2234,\n",
              "  2067,\n",
              "  2013,\n",
              "  1037,\n",
              "  4440,\n",
              "  2000,\n",
              "  3190,\n",
              "  1010,\n",
              "  4307,\n",
              "  1012,\n",
              "  2023,\n",
              "  13608,\n",
              "  11795,\n",
              "  18236,\n",
              "  2003,\n",
              "  2179,\n",
              "  2247,\n",
              "  1996,\n",
              "  5370,\n",
              "  1997,\n",
              "  2697,\n",
              "  4174,\n",
              "  1012,\n",
              "  2076,\n",
              "  2010,\n",
              "  3942,\n",
              "  1010,\n",
              "  6766,\n",
              "  2985,\n",
              "  1037,\n",
              "  2843,\n",
              "  1997,\n",
              "  2051,\n",
              "  11131,\n",
              "  1996,\n",
              "  2103,\n",
              "  2000,\n",
              "  3942,\n",
              "  2590,\n",
              "  16209,\n",
              "  1998,\n",
              "  10490,\n",
              "  1012,\n",
              "  102])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "encoded_input = tokenizer(sample_text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t290KponyQ3F",
        "outputId": "d45d4995-9dcd-4703-fafc-895ff04eb3b6"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3987,  0.0766, -0.7748,  ..., -0.0594,  0.6937, -0.4125],\n",
              "         [ 0.0349, -0.5916,  0.0343,  ...,  0.0782,  0.2416, -0.4458],\n",
              "         [ 0.2022, -0.4534,  0.0832,  ...,  0.0120,  0.0234, -0.2858],\n",
              "         ...,\n",
              "         [ 1.0440,  0.9648, -0.1495,  ..., -0.0479, -0.4928, -0.1610],\n",
              "         [-0.2571, -0.3461, -0.1741,  ...,  0.3672,  0.1976, -0.4908],\n",
              "         [-0.4563, -0.1745, -0.4159,  ...,  0.4302, -0.1528, -1.3966]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-7.6195e-01, -5.6361e-01, -9.9172e-01,  6.4513e-01,  8.3386e-01,\n",
              "         -1.5436e-01,  5.3941e-01,  3.7694e-01, -9.5500e-01, -9.9994e-01,\n",
              "         -7.7293e-01,  9.7995e-01,  9.7832e-01,  8.1757e-01,  8.0023e-01,\n",
              "         -5.5091e-01, -2.7588e-01, -5.3327e-01,  2.8927e-01,  6.7363e-01,\n",
              "          7.7889e-01,  1.0000e+00, -4.6736e-01,  3.6031e-01,  5.6643e-01,\n",
              "          9.9718e-01, -8.6944e-01,  8.7331e-01,  9.1776e-01,  5.8405e-01,\n",
              "         -4.3054e-01,  3.4974e-01, -9.9252e-01, -1.0982e-01, -9.9218e-01,\n",
              "         -9.8651e-01,  6.0920e-01, -4.2297e-01,  3.1164e-02, -5.3942e-03,\n",
              "         -7.1619e-01,  3.6778e-01,  9.9999e-01, -7.3103e-01,  6.3637e-01,\n",
              "         -1.4122e-01, -1.0000e+00,  2.7492e-01, -7.7013e-01,  9.8302e-01,\n",
              "          9.4400e-01,  9.8280e-01,  2.5756e-01,  5.4404e-01,  6.1767e-01,\n",
              "         -6.0250e-01, -7.5383e-02,  7.6878e-02, -2.4470e-01, -6.2140e-01,\n",
              "         -5.6703e-01,  3.7630e-01, -9.2433e-01, -8.0045e-01,  9.8075e-01,\n",
              "          9.4871e-01, -3.5928e-01, -4.9245e-01, -1.0327e-01, -2.5305e-01,\n",
              "          7.3790e-01,  3.8207e-01, -6.3014e-01, -8.9087e-01,  9.1709e-01,\n",
              "          3.5783e-01, -6.2397e-01,  1.0000e+00, -4.6803e-01, -9.6857e-01,\n",
              "          9.8823e-01,  9.3237e-01,  6.2843e-01, -5.4750e-01,  8.2071e-01,\n",
              "         -1.0000e+00,  6.4504e-01, -2.5133e-01, -9.8864e-01,  2.6983e-01,\n",
              "          7.3187e-01, -3.9094e-01,  9.7752e-01,  6.5142e-01, -6.8446e-01,\n",
              "         -7.0097e-01, -3.1276e-01, -9.7023e-01, -4.5510e-01, -5.6122e-01,\n",
              "          1.2755e-01, -4.5049e-01, -3.3265e-01, -3.1527e-01,  4.0269e-01,\n",
              "         -5.4525e-01,  1.1629e-01,  7.9970e-01,  7.9881e-02,  6.7429e-01,\n",
              "          4.3249e-01, -4.6510e-01,  4.6435e-01, -9.4563e-01,  6.1818e-01,\n",
              "         -3.5532e-01, -9.8917e-01, -5.9469e-01, -9.8949e-01,  6.3398e-01,\n",
              "         -4.7822e-01, -4.7408e-01,  9.0996e-01, -8.4722e-01,  5.3722e-01,\n",
              "         -1.2442e-01, -9.8909e-01, -1.0000e+00, -7.0873e-01, -4.7182e-01,\n",
              "         -4.2986e-01, -4.8248e-01, -9.6018e-01, -9.6518e-01,  5.3409e-01,\n",
              "          9.0893e-01,  4.1594e-01,  9.9993e-01, -5.5105e-01,  9.3308e-01,\n",
              "         -4.5532e-01, -7.0708e-01,  8.3791e-01, -6.0342e-01,  8.9458e-01,\n",
              "         -3.1694e-01,  2.0619e-01,  1.1192e-01, -6.9381e-01,  4.5532e-01,\n",
              "         -9.2989e-01, -4.2226e-01, -8.5960e-01, -8.3355e-01, -4.0855e-01,\n",
              "          8.9768e-01, -8.0076e-01, -9.8570e-01, -4.6924e-01, -1.9279e-01,\n",
              "         -3.6050e-01,  6.3155e-01,  8.7760e-01,  3.9697e-01, -5.3404e-01,\n",
              "          5.8049e-01,  8.6894e-02,  4.3821e-01, -7.1378e-01, -3.5983e-01,\n",
              "          4.6506e-01, -2.4490e-01, -9.8136e-01, -9.8223e-01, -3.3484e-01,\n",
              "          4.1387e-01,  9.8705e-01,  6.4449e-01,  3.3874e-01,  8.7337e-01,\n",
              "         -3.1170e-01,  6.0326e-01, -9.5809e-01,  9.8679e-01, -2.8704e-01,\n",
              "          3.7589e-01, -9.2184e-01,  8.7007e-01, -6.1933e-01,  2.4370e-01,\n",
              "          6.3784e-01, -8.2720e-01, -7.6557e-01, -3.3231e-01, -5.2633e-01,\n",
              "         -2.6337e-01, -9.7805e-01,  3.0954e-01, -3.2491e-01, -3.8140e-01,\n",
              "         -2.8704e-01,  9.1236e-01,  7.5337e-01,  3.4927e-01,  8.1204e-01,\n",
              "          3.7811e-01, -8.3473e-01, -4.3812e-01,  1.3091e-01,  2.3955e-01,\n",
              "          1.7627e-01,  9.9045e-01, -9.3583e-01, -1.7187e-01, -8.6375e-01,\n",
              "         -9.8573e-01, -2.2215e-01, -8.1404e-01, -2.5049e-01, -6.2004e-01,\n",
              "          7.4807e-01, -9.4024e-01,  6.7406e-01,  3.6776e-01, -5.3850e-01,\n",
              "         -7.0773e-01,  4.2560e-01, -6.0168e-01,  5.3196e-01, -1.3779e-01,\n",
              "          9.5131e-01,  9.8733e-01, -5.6621e-01, -6.4668e-01,  9.5421e-01,\n",
              "         -9.8657e-01, -8.1042e-01,  1.9725e-01, -3.4641e-01,  7.6301e-01,\n",
              "         -6.0922e-01,  9.6599e-01,  9.6323e-01,  8.2667e-01, -8.6950e-01,\n",
              "         -9.6190e-01, -1.9995e-01, -8.5788e-01, -2.5119e-01,  4.6552e-01,\n",
              "          9.7762e-01,  5.9593e-01,  5.2636e-01,  3.3475e-01, -5.2092e-01,\n",
              "          9.2945e-01, -9.7771e-01, -9.3567e-01, -9.8456e-01, -5.0752e-01,\n",
              "         -9.8718e-01,  9.7333e-01,  3.7966e-01,  8.7478e-01, -4.8668e-01,\n",
              "         -6.7634e-01, -9.3344e-01,  4.2312e-01,  1.6227e-01,  8.8187e-01,\n",
              "         -6.9186e-01, -5.3494e-01, -7.0996e-01, -9.3793e-01, -1.5481e-01,\n",
              "         -1.9429e-01, -8.5162e-01,  1.0945e-01, -8.6289e-01,  5.4050e-01,\n",
              "          5.3477e-01,  6.9222e-01, -9.8911e-01,  9.9125e-01,  1.0000e+00,\n",
              "          9.6889e-01,  8.0513e-01,  5.1951e-01, -1.0000e+00, -9.3132e-01,\n",
              "          9.9999e-01, -9.9873e-01, -1.0000e+00, -8.5608e-01, -6.4223e-01,\n",
              "          4.8195e-01, -1.0000e+00, -3.0236e-01,  2.4222e-02, -8.6273e-01,\n",
              "          8.9006e-01,  9.4544e-01,  9.1168e-01, -1.0000e+00,  6.8315e-01,\n",
              "          9.1185e-01, -5.5610e-01,  9.9055e-01, -5.8845e-01,  9.7380e-01,\n",
              "          4.4839e-01,  5.5710e-01, -2.9935e-01,  5.1799e-01, -9.9177e-01,\n",
              "         -7.5009e-01, -8.8536e-01, -9.7365e-01,  9.9997e-01,  4.4546e-02,\n",
              "         -7.0859e-01, -8.5440e-01,  8.4883e-01, -2.0215e-01, -1.4460e-04,\n",
              "         -9.4636e-01, -3.1443e-01,  7.9486e-01,  6.6082e-01,  4.2491e-01,\n",
              "          5.6793e-01, -4.6333e-01,  2.8842e-01,  6.5439e-01, -1.1485e-01,\n",
              "          6.1574e-01, -8.6516e-01, -3.6319e-01,  2.2919e-01,  2.3500e-01,\n",
              "         -8.5418e-01, -9.7002e-01,  9.2580e-01, -5.1699e-01,  9.2469e-01,\n",
              "          1.0000e+00,  8.3837e-01, -7.5195e-01,  7.6788e-01,  4.8670e-01,\n",
              "         -3.4268e-01,  1.0000e+00,  9.3183e-01, -9.7955e-01, -5.6747e-01,\n",
              "          8.9251e-01, -6.6041e-01, -7.4711e-01,  9.9880e-01, -4.1300e-01,\n",
              "         -9.2382e-01, -6.7850e-01,  9.8603e-01, -9.8599e-01,  9.9992e-01,\n",
              "         -7.3564e-01, -9.6502e-01,  9.6221e-01,  9.2089e-01, -7.0938e-01,\n",
              "         -4.4979e-01,  1.1974e-01, -7.9061e-01,  4.0324e-01, -5.8739e-01,\n",
              "          8.0805e-01,  2.5479e-01, -3.6166e-03,  8.0550e-01,  1.9183e-01,\n",
              "         -6.6280e-01,  2.8111e-01, -8.2701e-01, -4.7683e-01,  9.9452e-01,\n",
              "          6.0774e-01, -1.2357e-01,  1.2251e-01, -1.9618e-01, -8.7548e-01,\n",
              "         -9.5924e-01,  7.9968e-01,  1.0000e+00, -3.6742e-01,  9.7782e-01,\n",
              "         -5.9252e-01, -5.5933e-02,  1.7063e-01,  7.0934e-01,  6.3881e-01,\n",
              "         -4.1892e-01, -7.6498e-01,  9.4729e-01, -7.5416e-01, -9.9435e-01,\n",
              "          4.2611e-01,  2.7140e-01, -1.4937e-01,  1.0000e+00,  6.9797e-01,\n",
              "          1.4600e-01,  7.1602e-01,  9.9809e-01, -9.0819e-02, -4.8085e-02,\n",
              "          9.7530e-01,  9.7179e-01, -2.0843e-01,  5.5007e-01,  4.6561e-01,\n",
              "         -9.6912e-01, -3.1203e-01, -5.8024e-01,  1.3361e-01, -8.9722e-01,\n",
              "          1.1056e-01, -9.4399e-01,  9.3052e-01,  9.9571e-01,  4.9764e-01,\n",
              "          2.0280e-01,  8.4886e-01,  1.0000e+00, -9.9467e-01,  3.4133e-01,\n",
              "          9.3785e-01, -4.6146e-01, -1.0000e+00, -7.2316e-01, -5.8821e-01,\n",
              "         -3.6883e-02, -9.7412e-01, -3.3657e-01,  3.5655e-01, -9.4486e-01,\n",
              "          9.1908e-01,  8.8312e-01, -7.6797e-01, -9.7421e-01, -7.2968e-01,\n",
              "          4.6893e-01,  3.1719e-01, -9.9923e-01, -6.2581e-01, -5.6393e-01,\n",
              "          6.6834e-01, -3.0977e-01, -8.7702e-01, -5.0231e-01, -4.1820e-01,\n",
              "          4.7734e-01, -1.7889e-01,  6.2147e-01,  9.6790e-01,  8.6590e-01,\n",
              "         -9.8415e-01, -7.9538e-01, -2.7365e-01, -5.7662e-01,  7.7220e-01,\n",
              "         -4.9188e-01, -9.9325e-01, -2.5042e-01,  1.0000e+00, -2.2753e-01,\n",
              "          9.6545e-01,  6.3214e-01,  3.1503e-01, -3.4255e-01,  1.6621e-01,\n",
              "          9.9065e-01,  3.8966e-01, -8.8363e-01, -9.4487e-01,  9.5031e-01,\n",
              "         -3.4027e-01,  6.0929e-01,  9.4279e-01,  9.0724e-01,  7.2762e-01,\n",
              "          9.5299e-01,  2.8032e-01, -1.8077e-01,  1.5555e-01,  9.7177e-01,\n",
              "          5.0649e-02, -5.0060e-01, -5.3236e-01, -3.7932e-02, -5.0277e-01,\n",
              "          9.1351e-01,  1.0000e+00,  3.6661e-01,  7.9303e-01, -9.9184e-01,\n",
              "         -9.6898e-01, -7.8726e-01,  1.0000e+00,  8.5930e-01, -3.5729e-01,\n",
              "          7.5987e-01,  8.1913e-01, -1.9674e-01,  3.0144e-02, -2.5218e-01,\n",
              "         -3.4834e-01,  4.5795e-01, -1.2110e-02,  9.3622e-01, -7.1809e-01,\n",
              "         -9.7706e-01, -6.6686e-01,  4.6190e-01, -9.2338e-01,  1.0000e+00,\n",
              "         -6.5599e-01, -2.7480e-01, -2.5867e-01, -7.8166e-01, -9.8877e-01,\n",
              "          5.6172e-02, -9.7884e-01, -2.5341e-01,  2.8224e-01,  9.4129e-01,\n",
              "          3.6470e-01, -6.0838e-01, -7.3393e-01,  9.7707e-01,  7.6314e-01,\n",
              "         -9.8468e-01, -9.3610e-01,  9.3594e-01, -9.5674e-01,  7.1017e-01,\n",
              "          1.0000e+00,  4.3599e-01,  6.7777e-01,  2.9142e-01, -3.8648e-01,\n",
              "          4.7950e-01, -8.0491e-01,  6.6407e-01, -9.0707e-01, -4.7816e-01,\n",
              "         -4.3332e-01,  4.3734e-01,  4.9581e-03, -9.8174e-01,  5.5580e-01,\n",
              "          3.1885e-01, -5.4534e-01, -7.5429e-01, -1.7589e-01,  5.5260e-01,\n",
              "          7.9136e-01, -1.9019e-01, -4.0058e-02,  1.8994e-01, -2.8761e-02,\n",
              "         -8.2720e-01, -5.8120e-01, -5.9102e-01, -1.0000e+00,  4.8894e-01,\n",
              "         -1.0000e+00,  8.9307e-01,  6.3459e-01, -2.0440e-01,  7.1069e-01,\n",
              "          8.8401e-01,  9.3975e-01, -5.2940e-01, -9.6770e-01,  3.7835e-01,\n",
              "          6.5981e-01, -3.3365e-01, -7.0701e-01, -4.8629e-01,  5.3023e-01,\n",
              "         -7.8088e-02,  4.2329e-01, -8.8508e-01,  7.2745e-01, -4.4153e-01,\n",
              "          1.0000e+00,  2.7945e-01, -5.0390e-01, -5.8363e-01,  3.7205e-01,\n",
              "         -2.1180e-01,  1.0000e+00, -4.4569e-01, -9.5887e-01,  5.1142e-01,\n",
              "         -8.5050e-01, -6.3138e-01,  5.4396e-01, -9.1603e-02, -8.4273e-01,\n",
              "         -9.9034e-01,  8.8317e-01,  2.5939e-01, -6.8374e-01,  7.3091e-01,\n",
              "         -2.9355e-01, -4.6903e-01,  6.4973e-02,  9.8527e-01,  9.8650e-01,\n",
              "          7.9633e-01,  3.8412e-01, -9.2761e-01, -5.7014e-01,  9.3977e-01,\n",
              "          2.4845e-01, -3.1125e-01,  2.9269e-01,  1.0000e+00,  5.0138e-01,\n",
              "         -8.0146e-01,  2.3133e-01, -8.7464e-01, -2.4418e-01, -8.6008e-01,\n",
              "          5.3134e-01,  4.3574e-01,  8.7989e-01, -2.7989e-01,  9.2674e-01,\n",
              "         -9.7093e-01,  9.0518e-02, -8.9943e-01, -8.0907e-01,  4.4548e-01,\n",
              "         -8.6019e-01, -9.7920e-01, -9.7109e-01,  7.7520e-01, -2.2628e-01,\n",
              "         -1.3880e-01,  3.0954e-01, -1.4632e-01,  4.1983e-01,  5.5290e-01,\n",
              "         -1.0000e+00,  9.0629e-01,  5.6138e-01,  9.7985e-01,  9.6749e-01,\n",
              "          8.4840e-01,  7.9622e-01,  3.7885e-01, -9.7008e-01, -8.2885e-01,\n",
              "         -4.1561e-01, -1.6860e-01,  4.5536e-01,  6.4038e-01,  8.2133e-01,\n",
              "          3.5871e-01, -6.1688e-01, -8.2435e-01, -8.6567e-01, -9.7905e-01,\n",
              "         -9.8645e-01,  5.8985e-01, -7.8209e-01, -5.3602e-01,  9.3534e-01,\n",
              "          1.0749e-01,  1.3683e-02, -5.1298e-01, -9.4404e-01,  4.3408e-01,\n",
              "          6.4507e-01, -1.9106e-01,  6.7691e-03,  4.4397e-01,  7.5268e-01,\n",
              "          7.3789e-01,  9.6755e-01, -9.8184e-01,  4.2722e-01, -9.6806e-01,\n",
              "          6.4407e-01,  9.7742e-01, -9.1140e-01,  1.3069e-01,  6.8876e-01,\n",
              "         -4.8231e-01,  2.1154e-01, -3.3207e-01, -5.7650e-01,  8.8399e-01,\n",
              "         -3.2909e-01,  7.0431e-01, -3.1282e-01, -4.5867e-02, -4.1399e-01,\n",
              "         -1.4233e-01, -6.5921e-01, -7.4814e-01,  6.6226e-01,  3.3714e-01,\n",
              "          7.5899e-01,  9.8325e-01, -1.7712e-01, -5.7421e-01, -2.3838e-01,\n",
              "         -9.2579e-01, -8.8619e-01,  4.1239e-01,  2.7384e-02, -3.9687e-01,\n",
              "          8.9449e-01, -2.3716e-01,  9.9439e-01,  6.3098e-01, -4.7036e-01,\n",
              "         -3.3902e-01, -6.0786e-01,  7.6535e-01, -8.0626e-01, -5.6439e-01,\n",
              "         -6.2849e-01,  8.5097e-01,  4.5109e-01,  1.0000e+00, -9.5917e-01,\n",
              "         -9.7380e-01, -6.5796e-01, -5.7491e-01,  5.0252e-01, -7.4001e-01,\n",
              "         -1.0000e+00,  3.0704e-01, -9.0668e-01,  9.2564e-01, -7.4235e-01,\n",
              "          9.8792e-01, -7.0567e-01, -8.7543e-01, -4.0668e-01,  8.6748e-01,\n",
              "          8.9695e-01, -4.5345e-01, -4.3859e-01,  5.4021e-01, -8.0620e-01,\n",
              "          9.9780e-01,  7.7456e-01, -9.0326e-01, -1.9968e-01,  6.4436e-01,\n",
              "         -9.1669e-01, -6.3525e-01,  7.0829e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4) Sentiment Analysis with Transformers"
      ],
      "metadata": {
        "id": "zFLlU3F7eXfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "text = \"I love this movie!\"\n",
        "result = pipe(text)\n",
        "print(f'{text} {result}')\n",
        "\n",
        "text2 = \"I hate it\"\n",
        "result = pipe(text2)\n",
        "print(f'{text2} {result}')"
      ],
      "metadata": {
        "id": "nHGL_PfWCNkj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468f0c88-a858-4e50-8b5d-4261712ca067"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love this movie! [{'label': 'POSITIVE', 'score': 0.9998775720596313}]\n",
            "I hate it [{'label': 'NEGATIVE', 'score': 0.9996398687362671}]\n"
          ]
        }
      ]
    }
  ]
}